{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nltk import word_tokenize, pos_tag, FreqDist, SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from gensim import corpora, models\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "userComments1 = pd.read_csv('C:/Users/Michael/Documents/GitHub/EBAC/dataSources/videoGames/metacritic_game_user_comments (0-100k).csv')\n",
    "userComments2 = pd.read_csv('C:/Users/Michael/Documents/GitHub/EBAC/dataSources/videoGames/metacritic_game_user_comments (100-200k).csv')\n",
    "userComments3 = pd.read_csv('C:/Users/Michael/Documents/GitHub/EBAC/dataSources/videoGames/metacritic_game_user_comments (200-300k).csv')\n",
    "\n",
    "userComments3['Unnamed: 0'] = userComments3['Unnamed: 0'].astype('float64')\n",
    "userComments3['Userscore'] = userComments3['Userscore'].astype('float64')\n",
    "\n",
    "userComments = pd.concat([userComments1, userComments2, userComments3], axis = 0)\n",
    "userComments.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "userComments.dropna(how = 'all', inplace = True)\n",
    "\n",
    "userComments.Platform.unique()\n",
    "\n",
    "platformMap = {\n",
    "'Nintendo64': 'Nintendo',\n",
    "'GameBoyAdvance': 'Nintendo',\n",
    "'3DS': 'Nintendo',\n",
    "'DS': 'Nintendo',\n",
    "'Wii': 'Nintendo',\n",
    "'Switch': 'Nintendo',\n",
    "'WiiU': 'Nintendo',\n",
    "'GameCube': 'Nintendo',\n",
    "'PlayStation': 'PlayStation',\n",
    "'PlayStation3': 'PlayStation',\n",
    "'PlayStation2': 'PlayStation',\n",
    "'PlayStation4': 'PlayStation',\n",
    "'PlayStationVita': 'PlayStation',\n",
    "'PSP': 'PlayStation',\n",
    "'Xbox360': 'Xbox',\n",
    "'Xbox': 'Xbox',\n",
    "'XboxOne': 'Xbox',\n",
    "'PC': 'PC',\n",
    "'Dreamcast': 'Others',\n",
    "'not specified': 'Others'\n",
    "}\n",
    "\n",
    "userComments['platformCondensed'] = userComments.Platform.map(platformMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = [\"would\", \"could\", \"left\", \"right\", \"a.m.\", \"p.m.\", \"'s\", \"! ! !\", \"...\", \":\", \";\", \"n't\",\n",
    "              \"game\", \"games\", \"play\", \"fun\", \"much\", \"one\", \"great\", \"perfect\", \"time\", \"year\", \"lot\", \"thing\", \"etc\",\n",
    "              \"hour\", \"hours\", \"way\", \"ways\", \"everything\", \"anything\", \"thing\", \"review\", \"year\", \"years\", \"feel\", \"feels\",\n",
    "              \"thing\", \"nothing\", \"problem\", \"end\", \"begin\", \"kind\", \"piece\", \"work\", \"call\", \"anyone\", \"minute\", \"minutes\",\n",
    "              \"waste\", \"crap\", \"garbage\"]\n",
    "\n",
    "def preprocess_text(tokens, needtokenizeBoolean = True, grams = False,  ngramsNumber = 2, furtherPreProcessNgrams = False):\n",
    "    \n",
    "    if needtokenizeBoolean:\n",
    "        tokens = nltk.word_tokenize(tokens)\n",
    "        if grams:\n",
    "            tokens = list(ngrams(tokens, ngramsNumber))\n",
    "    if grams:\n",
    "        tokens = [' '.join(gram) for gram in tokens]\n",
    "        if furtherPreProcessNgrams == False:\n",
    "            return tokens\n",
    "\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english') + filter_list]\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens = [t for t in tokens if not t.isnumeric()]\n",
    "    tokens = [SnowballStemmer('english').stem(t) for t in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def wc(df, columnName, preProcessingFunctionBoolean = True, tfidfVectorizerBoolean = True, vectorizerMinDf = 2, vectorizerMaxDf = 0.7, countVectorizerBinary = True,\n",
    "       nounTaggingBoolean = False, universalNounTagsetBoolean = False, ngrams = False, ngramsNumber = 2, furtherPreProcessNgrams = False,\n",
    "       top = 10, features_improvements = 'Word Cloud', platform = 'All'):\n",
    "\n",
    "    if preProcessingFunctionBoolean == True & ngrams == True:\n",
    "        if tfidfVectorizerBoolean == True:\n",
    "            if furtherPreProcessNgrams == True:\n",
    "                tdm = TfidfVectorizer(tokenizer = lambda text: preprocess_text(text, grams = True, ngramsNumber=ngramsNumber, furtherPreProcessNgrams = True), min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "            else:\n",
    "                tdm = TfidfVectorizer(tokenizer = lambda text: preprocess_text(text, grams = True, ngramsNumber=ngramsNumber), min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "        else:\n",
    "            if furtherPreProcessNgrams == True:\n",
    "                tdm = CountVectorizer(binary = countVectorizerBinary, tokenizer = lambda text: preprocess_text(text, grams = True, ngramsNumber=ngramsNumber, furtherPreProcessNgrams = True), min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "            else:\n",
    "                tdm = CountVectorizer(binary = countVectorizerBinary, tokenizer = lambda text: preprocess_text(text, grams = True, ngramsNumber=ngramsNumber), min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "        tdmMatrix = tdm.fit_transform(df[columnName])\n",
    "    \n",
    "    if nounTaggingBoolean == True and ngrams == False:\n",
    "        noun_list = []\n",
    "        for value in df[columnName]:\n",
    "            if universalNounTagsetBoolean == True:\n",
    "                tagged_value = pos_tag(word_tokenize(value), tagset='universal')\n",
    "                noun = [word for word, tag in tagged_value if tag == 'NOUN']\n",
    "            else:\n",
    "                tagged_value = pos_tag(word_tokenize(value))\n",
    "                noun = [word for word, tag in tagged_value if tag == 'NN' or tag == 'NNS']\n",
    "            noun_list.append(noun)\n",
    "        df['Text_NounOnly'] = noun_list\n",
    "        if preProcessingFunctionBoolean == True:\n",
    "            df['Text_NounOnly'] = df['Text_NounOnly'].apply(lambda x: preprocess_text(tokens = x, needtokenizeBoolean = False))\n",
    "            df['Text_NounOnly'] = df['Text_NounOnly'].apply(lambda x: ', '.join(x))\n",
    "        else:\n",
    "            df['Text_NounOnly'] = df['Text_NounOnly'].apply(lambda x: ', '.join(x))\n",
    "        if tfidfVectorizerBoolean == True:\n",
    "            tdm = TfidfVectorizer(min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "        else:\n",
    "            tdm = CountVectorizer(binary = countVectorizerBinary, min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "        tdmMatrix = tdm.fit_transform(df['Text_NounOnly'])\n",
    "        \n",
    "    if nounTaggingBoolean != True and ngrams == False:\n",
    "        if tfidfVectorizerBoolean == True:\n",
    "            if preProcessingFunctionBoolean == True:\n",
    "                tdm = TfidfVectorizer(tokenizer = preprocess_text, min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "            else:\n",
    "                tdm = TfidfVectorizer(min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "        else:\n",
    "            if preProcessingFunctionBoolean == True:\n",
    "                tdm = CountVectorizer(binary = countVectorizerBinary, tokenizer = preprocess_text, min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "            else:\n",
    "                tdm = CountVectorizer(binary = countVectorizerBinary, min_df = vectorizerMinDf, max_df = vectorizerMaxDf)\n",
    "        tdmMatrix = tdm.fit_transform(df[columnName])\n",
    "\n",
    "    array = tdmMatrix.toarray()\n",
    "    feature_names = tdm.get_feature_names_out()\n",
    "    word_tfidf = dict(zip(feature_names, array.sum(axis=0)))\n",
    "    fd_tfidf= FreqDist(word_tfidf)\n",
    "    wc = WordCloud(background_color=\"white\").generate_from_frequencies(fd_tfidf)\n",
    "    plt.figure()\n",
    "    plt.suptitle(f\"{features_improvements} ({platform})\", fontsize = 20, x = 0.5, y = 0.85, fontweight = 'bold', fontname = 'Calibri') \n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    displayList = []\n",
    "    for x,y in fd_tfidf.most_common(top):\n",
    "        displayList.append((x, round(y, 2)))\n",
    "\n",
    "    plt.show()\n",
    "    print(displayList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283983, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userComments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCommentsTEST = userComments.sample(283983).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Userscore\n",
       "10.0    112531\n",
       "9.0      53489\n",
       "8.0      30271\n",
       "7.0      17052\n",
       "6.0      12036\n",
       "5.0      10113\n",
       "4.0       7952\n",
       "3.0       7118\n",
       "2.0       5983\n",
       "1.0       7318\n",
       "0.0      20120\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userCommentsTEST.Userscore.value_counts().sort_index(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: Game creators will want to focus on problems where they can fix and do better than improving what's already good (more weights on low userscores?)\n",
    "\n",
    "userCommentsTESTNotNull = userCommentsTEST[userCommentsTEST.Comment.notnull()]\n",
    "\n",
    "userCommentsTESTNotNull.loc[userCommentsTESTNotNull.Userscore > 9, 'scoreBin'] = 'High'\n",
    "userCommentsTESTNotNull.loc[(userCommentsTESTNotNull.Userscore >= 3) & (userCommentsTESTNotNull.Userscore <= 9), 'scoreBin'] = 'Medium'\n",
    "userCommentsTESTNotNull.loc[userCommentsTESTNotNull.Userscore < 3, 'scoreBin'] = 'Low'\n",
    "\n",
    "userCommentsTESTHigh = userCommentsTESTNotNull[userCommentsTESTNotNull.scoreBin == 'High']\n",
    "userCommentsTESTLow = userCommentsTESTNotNull[userCommentsTESTNotNull.scoreBin == 'Low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC, High Userscore; No. of comments: 43259\n",
      "PC, Low Userscore; No. of comments: 18236\n",
      "PlayStation, High Userscore; No. of comments: 34452\n",
      "PlayStation, Low Userscore; No. of comments: 7781\n",
      "Nintendo, High Userscore; No. of comments: 16357\n",
      "Nintendo, Low Userscore; No. of comments: 1009\n",
      "Xbox, High Userscore; No. of comments: 18096\n",
      "Xbox, Low Userscore; No. of comments: 6371\n",
      "Others, High Userscore; No. of comments: 358\n",
      "Others, Low Userscore; No. of comments: 19\n"
     ]
    }
   ],
   "source": [
    "platformCondensed_list = list(userCommentsTESTNotNull[userCommentsTESTNotNull.scoreBin != 'Medium'].platformCondensed.unique())\n",
    "userCommentsTESTExtreme_list = [userCommentsTESTHigh, userCommentsTESTLow]\n",
    "\n",
    "for platform in platformCondensed_list:\n",
    "    for df in userCommentsTESTExtreme_list:\n",
    "        print(f\"{platform}, {df.reset_index().scoreBin[0]} Userscore; No. of comments: {df[df.platformCondensed == platform].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Can adjust the parameters in this function to fine tune and iterate testings. Can adjust score bins, sample size and filter list above too!\n",
    "# If ngrams = True, preProcessingFunctionBoolean needs to be True\n",
    "\n",
    "for df in userCommentsTESTExtreme_list:\n",
    "    for platform in platformCondensed_list:\n",
    "            if df.scoreBin.max() == 'High':\n",
    "                features_improvements = 'Key Features'\n",
    "            elif df.scoreBin.max() == 'Low':\n",
    "                features_improvements = 'Improvements Needed'\n",
    "            try:\n",
    "                wc(df = df[df.platformCondensed == platform], columnName = 'Comment',\n",
    "                preProcessingFunctionBoolean = True, vectorizerMinDf = 2, vectorizerMaxDf = 0.7,\n",
    "                countVectorizerBinary = True, tfidfVectorizerBoolean = True,\n",
    "                ngrams = False, ngramsNumber = 3, furtherPreProcessNgrams = False,\n",
    "                nounTaggingBoolean = True, universalNounTagsetBoolean = False,\n",
    "                top = 20, features_improvements = features_improvements, platform = platform)\n",
    "            except ValueError:\n",
    "                print(f\"\\n\\nNo Word Cloud for '{features_improvements} ({platform})' due to insufficient sample size (No. of comments = 0 or < vectorizerMinDf).\\n\\n\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
